{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bitf1a6e18b803b4647aaf735937bafcce9",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Airflow\n",
    "purposes: data engineering work\n",
    "### What is a workflow?\n",
    "A workflow is:\n",
    "* A set of steps to accomplis a given data engineering task\n",
    "    * Such as: ETL stuff. Download, copying, filtering data, then pushing to database\n",
    "* Of varying levels of complexity (2-3 steps or hundreds)\n",
    "\n",
    "### What is Airflow?\n",
    "*Airflow* is a platform to program workflows, including:\n",
    "* Creation\n",
    "* Scheduling\n",
    "* Monitoring\n",
    "* Airflow can use various tools/languages but the actual workflow code is written with Python\n",
    "* Implements workflows as DAGs: Directed Acyclic Graphs (For now, think of this like a set of tasks and the dependencies between them)\n",
    "* Accessed via code, command-line, or via web interface\n",
    "\n",
    "### Other workflow tools\n",
    "* Luigi (Spotify's tool)\n",
    "* SSIS (Microsoft Sql Server Integration Services)\n",
    "* Bash scripting (we'll use some of this in this tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  DAG Code example \n",
    "#  Simple DAG defintion:\n",
    "etl_dag = DAG(\n",
    "    dag_id='etl_pipeline',\n",
    "    default_args={'start_date': '2020-01-08'}\n",
    ")\n",
    "\n",
    "#  Note: within any Python code this is referred to via the variable \"etl_dag\" but via command line, need to use the dag_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a workflow in Airflow\n",
    "Running a simple Airflow task in bash terminal\n",
    "\n",
    "``` airflow run <dag_id> <task_id> <start_date> ```\n",
    "\n",
    "Using a DAG named example-etl, a task named download-file, and a start date of '2020-01-10':\n",
    "\n",
    "``` airflow run example-etl download-file 2020-01-10 ```\n",
    "\n",
    "you can't edit DAGs from Airflow subcommands in the terminal. Use airflow -h for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a DAG?\n",
    "DAG, or Directed Acyclic Graph:\n",
    "* Directed, there is an inherent flow representing dependencies between components\n",
    "* Acyclic, does not loop/cycle/repeat\n",
    "* Graph, the actual set of components\n",
    "* Seen in Airflow, Apache Spark, Luigi\n",
    "\n",
    "Within Airflow, DAGs:\n",
    "* Are written in Python (but can use components written in other languages)\n",
    "* Are made up of components (typically, tasks) to be executed, such as operators, sensors, etc.\n",
    "* Contain dependencies defined explicity or implicity\n",
    "    * ie, Copy the file to the server before trying to import it to the database service\n",
    "\n",
    "## Define a DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "\n",
    "#  These arguments are optional but provide a lot of power to define the runtime behavior of Airflow\n",
    "default_arguments = {\n",
    "    'owner': 'Fausto Rodriguez',\n",
    "    'email': 'cozyscripts@github.io',\n",
    "    'start_date': datetime(2020, 8, 5)\n",
    "}\n",
    "\n",
    "#  Define DAG object with the first argument using a name for the DAG, etl_workflow\n",
    "#  Assign the default arguments dictionary to the default_args argument\n",
    "etl_dag = DAG('etl_workflow', default_args=default_arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The DAG is assigned to variable \"etl_dag\". This variable will be used when defining components of the DAG. However, \"etl_dag\" will not appear in the Airflow interfaces.\n",
    "\n",
    "### DAGs on the command line:\n",
    "Generally, you'll want to use the airflow command line tool\n",
    "\n",
    "Using ```airflow```:\n",
    "* The ```airflow``` command line program contains many subcommands\n",
    "* ```airflow -h``` for descriptions\n",
    "* Many subcommands are related to DAGs\n",
    "* ```airflow list_dags``` to show all recognized DAGs in an installation\n",
    "\n",
    "## Command line vs. Python\n",
    "Use the command line tool to:\n",
    "* Start Airflow processes\n",
    "* Manually run DAGs/Tasks\n",
    "* Get logging information from Airflow\n",
    "\n",
    "Use Python to:\n",
    "* Create a DAG\n",
    "* Edit the individual properties of a DAG\n",
    "* Edit the actual data processing code itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}